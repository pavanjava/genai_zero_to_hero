{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fee1e7-081e-4c22-af70-9839fcfcec32",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336b23c-be24-4299-82d1-4ba0c578385d",
   "metadata": {},
   "source": [
    "### Import helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2df73149-4fad-4f63-aa85-d338cbffe23d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T05:24:42.701915Z",
     "start_time": "2025-02-08T05:24:42.695342Z"
    },
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faef740-19bd-421e-a985-43217d6d6825",
   "metadata": {},
   "source": [
    "### In-Context Learning\n",
    "\n",
    "#### Standard prompt with instruction\n",
    "- So far, you have been stating the instruction explicitly in the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ea534c7-e213-417f-a99c-0f8dba92387e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T05:24:48.242395Z",
     "start_time": "2025-02-08T05:24:47.118990Z"
    },
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this message is extremely positive. The use of the word \"thoughtful\" to describe the lecture and the expression of gratitude with \"thanks\" convey a strong sense of appreciation and admiration for the speaker's effort.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the sentiment of:\n",
    "Hi Pavan, thanks for the thoughtful Lecture today!\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45096c64-f483-4d01-b64c-8de1fb529441",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting\n",
    "- Here is an example of zero-shot prompting.\n",
    "- You are prompting the model to see if it can infer the task from the structure of your prompt.\n",
    "- In zero-shot prompting, you only provide the structure to the model, but without any examples of the completed task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "549667d6-6b23-46a9-8c4a-f089f7936392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T05:28:27.949050Z",
     "start_time": "2025-02-08T05:28:26.455468Z"
    },
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of this message is positive. The user is expressing gratitude and appreciation for the lecture given by Pavan, indicating a favorable tone.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Pavan, thanks for the thoughtful Lecture today!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162fb3d-7020-47c6-bb08-5ed6f138da66",
   "metadata": {},
   "source": [
    "### Few-shot Prompting\n",
    "- Here is an example of few-shot prompting.\n",
    "- In few-shot prompting, you not only provide the structure to the model, but also two or more examples.\n",
    "- You are prompting the model to see if it can infer the task from the structure, as well as the examples in your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd349402-904a-4e33-9e7c-88a41afc5ec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T05:29:43.161443Z",
     "start_time": "2025-02-08T05:29:39.516228Z"
    },
    "height": 217
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the sentiment of the third message, we need to analyze its content.\n",
      "\n",
      "The message reads: \"Hi Pavan, thanks for the thoughtful Lecture today!\"\n",
      "\n",
      "In this message:\n",
      "\n",
      "- The speaker is expressing gratitude towards you (Pavan) for something.\n",
      "- They are using positive adjectives like \"thoughtful\" to describe your lecture.\n",
      "- There's no negative language or tone in the message.\n",
      "\n",
      "Based on these observations, I would classify the sentiment of the third message as: {\"intent\": \"Positive\"}\n",
      "\n",
      "So, the updated list would be:\n",
      "\n",
      "1. Sentiment: {\"intent\": \"Negative\"} (Message 1)\n",
      "2. Sentiment: {\"intent\": \"Positive\"} (Message 2)\n",
      "3. Sentiment: {\"intent\": \"Positive\"} (Message 3)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Pavan, you're 20 minutes late to my AI Lecture!\n",
    "Sentiment: {\"intent\": \"Negative\"}\n",
    "\n",
    "Message: Can't wait to order a book for my birthday\n",
    "Sentiment: {\"intent\": \"Positive\"}\n",
    "\n",
    "Message: Hi Pavan, thanks for the thoughtful Lecture today!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41560853-9639-4329-87ac-640620bb4b39",
   "metadata": {},
   "source": [
    "### Specifying the Output Format\n",
    "- You can also specify the format in which you want the model to respond.\n",
    "- In the example below, you are asking to \"give a one word response\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f2669f8-9673-4568-aec2-4b9f13d033ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T05:36:48.815827Z",
     "start_time": "2025-02-08T05:36:27.197332Z"
    },
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the sentiment of the message \"Hi Pavan, thanks for the thoughtful Lecture today!\" The user is asking me to respond with just one word. Let's break this down step by step.\n",
      "\n",
      "First, looking at the structure of the message: it starts with a greeting (\"Hi Pavan\"), which is pretty standard and doesn't carry any negative or positive connotations on its own. Then there's an expression of gratitude: \"thanks for the thoughtful Lecture today!\" The word \"thoughtful\" here is key because it's describing something positive about the lecture. So, the speaker is appreciative and acknowledging that the lecture was well-considered or well-prepared.\n",
      "\n",
      "Now, considering the context from the previous messages provided. In the first message, the sentiment was negative because the person was upset about being late to an AI Lecture. The second message was positive because they were excited to order a book for their birthday. So, this third message is in response to the AI Lecture, which might be related to the first one where there was a delay.\n",
      "\n",
      "Putting it all together, the speaker is thanking Pavan for a thoughtful lecture today. Since \"thoughtful\" is a positive adjective, the sentiment here should be positive. The user wants a one-word response, so I need to choose a word that reflects this positivity without being too strong or out of context.\n",
      "\n",
      "Possible words could be \"Thanks,\" \"Appreciative,\" \"Grateful,\" or \"Positive.\" But since the instruction is for a one-word response and considering the previous examples where \"Thanks\" was used in positive contexts, maybe \"Thanks\" fits here. However, looking back at the first example, when the sentiment was negative, the response was just \"Negative.\" So perhaps the correct approach is to mirror that structure.\n",
      "\n",
      "Wait, no, in the initial examples, the user provided three messages with sentiments: Negative, Positive, and then a question mark. The third one didn't have a sentiment assigned, so the task was to determine it. In this case, the message is positive, so the response should be \"Positive.\"\n",
      "\n",
      "Yes, that makes sense because the speaker is expressing gratitude in a positive manner by calling the lecture thoughtful.\n",
      "</think>\n",
      "\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Pavan, you're 20 minutes late to my AI Lecture!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order a book for my birthday\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Pavan, thanks for the thoughtful Lecture today!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt=prompt, model=\"deepseek-r1:8b\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e07176-5730-4cc0-bec1-cb3ae9000806",
   "metadata": {},
   "source": [
    "**Note:** For all the examples above, you used the 7 billion parameter model, `llama-2-7b-chat`. And as you saw in the last example, the 7B model was uncertain about the sentiment.\n",
    "\n",
    "- You can use the larger (70 billion parameter) `llama-2-70b-chat` model to see if you get a better, certain response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415739fe-dcfe-4183-a5d8-fa3ce19aa480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:38:41.933138Z",
     "start_time": "2025-02-07T16:38:20.559942Z"
    },
    "height": 268
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out the sentiment of the message \"Hi Pavan, thanks for the thoughtful Lecture today!\" The user is asking me to respond with just one word. Let's break this down step by step.\n",
      "\n",
      "First, looking at the structure of the message: it starts with a greeting (\"Hi Pavan\"), which is pretty standard and doesn't carry any negative or positive connotations on its own. Then there's an expression of gratitude: \"thanks for the thoughtful Lecture today!\" The word \"thoughtful\" here is key because it's describing something positive about the lecture. So, the speaker is appreciative and acknowledging that the lecture was well-considered or well-prepared.\n",
      "\n",
      "Now, considering the context from the previous messages provided. In the first message, the sentiment was negative because the person was upset about being late to an AI Lecture. The second message was positive because they were excited to order a book for their birthday. So, this third message is in response to the AI Lecture, which might be related to the first one where there was a delay.\n",
      "\n",
      "Putting it all together, the speaker is thanking Pavan for a thoughtful lecture today. Since \"thoughtful\" is a positive adjective, the sentiment here should be positive. The user wants a one-word response, so I need to choose a word that reflects this positivity without being too strong or out of context.\n",
      "\n",
      "Possible words could be \"Thanks,\" \"Appreciative,\" \"Grateful,\" or \"Positive.\" But since the instruction is for a one-word response and considering the previous examples where \"Thanks\" was used in positive contexts, maybe \"Thanks\" fits here. However, looking back at the first example, when the sentiment was negative, the response was just \"Negative.\" So perhaps the correct approach is to mirror that structure.\n",
      "\n",
      "Wait, no, in the initial examples, the user provided three messages with sentiments: Negative, Positive, and then a question mark. The third one didn't have a sentiment assigned, so the task was to determine it. In this case, the message is positive, so the response should be \"Positive.\"\n",
      "\n",
      "Yes, that makes sense because the speaker is expressing gratitude in a positive manner by calling the lecture thoughtful.\n",
      "</think>\n",
      "\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Pavan, you're 20 minutes late to my AI Lecture!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order a book for my birthday\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Pavan, thanks for the thoughtful Lecture today!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt,model=\"deepseek-r1:8b\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe196c59-2c42-4bdb-b7e7-a2f431ffce29",
   "metadata": {},
   "source": [
    "- Now, use the smaller model again, but adjust your prompt in order to help the model to understand what is being expected from it.\n",
    "- Restrict the model's output format to choose from `positive`, `negative` or `neutral`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74528daa-90be-4208-98ac-3cf71dc687b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T05:40:14.527272Z",
     "start_time": "2025-02-08T05:40:11.885820Z"
    },
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral. The sentiment of the message is not explicitly stated as positive or negative, but rather it's a polite and appreciative greeting.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Pavan, you're 20 minutes late to my AI Lecture!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order a book for my birthday\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Pavan, thanks for the thoughtful Lecture today!\n",
    "Sentiment: ?\n",
    "\n",
    "Respond with either positive, negative, or neutral.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3d067-2b77-4a35-b95b-d7ed9c5bc77f",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "- Roles give context to LLMs what type of answers are desired.\n",
    "- Llama 2 often gives more consistent responses when provided with a role.\n",
    "- First, try standard prompt and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a94c6ba-5dd7-4a60-9dfe-c878c44fa44f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T05:41:22.538336Z",
     "start_time": "2025-02-08T05:41:16.088615Z"
    },
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer, also known as a wordpiece or subword tokenizer, is a crucial component of Large Language Models (LLMs). Here's how you can answer your friend's question:\n",
      "\n",
      "**What is the tokenizer in LLM?**\n",
      "\n",
      "In the context of Large Language Models, the tokenizer is a module that splits input text into individual tokens. These tokens are then fed into the model as input.\n",
      "\n",
      "Think of it like this: when you type a sentence on your keyboard, each key press creates a token (e.g., \"hello\" becomes 3 tokens: h-e-l-lo). The tokenizer does something similar with text input. It breaks down words and subwords (smaller units within words) into individual tokens that the model can process.\n",
      "\n",
      "The tokenizer's primary function is to:\n",
      "\n",
      "1. **Split words into subwords**: Large language models often use subwording techniques, like WordPiece or BPE (Byte Pair Encoding), which split words into smaller subwords based on their frequency and context.\n",
      "2. **Convert text to numerical representations**: The tokenizer converts each token into a unique numerical representation, allowing the model to process and manipulate the input data efficiently.\n",
      "\n",
      "The output of the tokenizer is typically a list of tokens, where each token corresponds to a specific character or subword in the original input text. This output is then fed into the LLM as input, enabling it to generate responses, answer questions, or perform other tasks.\n",
      "\n",
      "In summary, the tokenizer plays a vital role in preparing input text for Large Language Models by breaking down words and subwords into individual tokens that can be processed and understood by the model.\n",
      "\n",
      "Hope this helps you answer your friend's question!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "How can I answer this question from my friend:\n",
    "What is the tokenizer in LLM?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42594189-5692-4ab3-9a1b-3d6ac58c8a78",
   "metadata": {},
   "source": [
    "- Now, try it by giving the model a \"role\", and within the role, a \"tone\" using which it should respond with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2db7b960-d6f6-48a0-8c62-a68c999ad4fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T05:47:23.480899Z",
     "start_time": "2025-02-08T05:47:17.677378Z"
    },
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta, tokenizer ek bahut hi important component hai LLM (Large Language Model) mein. \n",
      "\n",
      "Tokenization ka matlab yah hai ki hum kisi bhi text ko small chunks mein todne ke liye ek algorithm ka upyog karte hain. Yeh chunks humein model ko understand karne aur process karne mein madad karte hain.\n",
      "\n",
      "Ek tokenizer ka kaam yah hai ki wo input text ko words, subwords ya tokens mein badal dete hain. Isse model ko samajhne aur process karne mein asaan hota hai. \n",
      "\n",
      "Ab, aap apni friend se puch sakte hain ki tokenizer kya hota hai? Kya yah ek machine learning algorithm hai? Kya yah humein input text ko understand karne mein madad karta hai?\n",
      "\n",
      "Aapke paas kuch examples bhi de sakte hain, jaise ki \"tokenizer ka upyog kaise hota hai?\" ya \"tokenizer ke liye kya algorithms upyogi hote hain?\"\n",
      "\n",
      "Is tarah se aap apni friend ko tokenizer ke baare mein samajhne mein madad kar sakte hain.\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"\n",
    "Your role is a AI Architect \\\n",
    "who gives advice to people about advanced architectural questions.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the tone of an hindi speaking professor.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "What is the tokenizer in LLM ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0169c84-8f68-4544-8ee1-d02265635d49",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "- Summarizing a large text is another common use case for LLMs. Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434cd16f-ef7f-4a6b-9076-007893848518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:41:41.457075Z",
     "start_time": "2025-02-07T16:41:41.453183Z"
    },
    "height": 404
   },
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Professor,\n",
    "\n",
    "I'm writing to share our comprehensive research findings on optimizing Retrieval Augmented Generation (RAG) pipelines. Our team has made significant breakthroughs in improving accuracy and reducing hallucination rates through systematic enhancements across multiple components of the RAG architecture.\n",
    "\n",
    "Our first major advancement came through chunking optimization. Traditional approaches often rely on arbitrary text splits, but we've implemented semantic chunking that preserves contextual meaning. By reducing chunk sizes to 256 tokens and incorporating a 20% overlap between adjacent chunks, we've maintained contextual continuity while enabling more precise retrieval. This modification alone yielded a 15% improvement in retrieval precision across our test cases.\n",
    "\n",
    "The vector database layer presented another opportunity for substantial improvement. We developed a hybrid search architecture that combines dense and sparse vectors, leveraging the strengths of both approaches. The integration of BM25 with cross-encoder reranking has proven particularly effective. By adding metadata filtering capabilities, we've enabled more nuanced domain-specific retrieval. These enhancements collectively reduced our hallucination rate by 27%, a significant improvement in output reliability.\n",
    "\n",
    "Query processing emerged as a critical component for overall system performance. We deployed an advanced query expansion system using a fine-tuned T5 model, which better captures semantic variations in user queries. Our hypothesis-driven query decomposition approach breaks complex queries into manageable sub-queries, while maintaining logical relationships. The implementation of query-specific routing to specialized indexes has further refined our retrieval accuracy. These query processing improvements resulted in a 32% boost in answer relevance.\n",
    "\n",
    "To validate these improvements, we developed a comprehensive evaluation framework. Our test suite now includes over 5,000 carefully curated question-answer pairs, spanning multiple domains and complexity levels. We employ automated evaluation metrics including ROUGE, BLEU, and BERTScore, supplemented by a structured human-in-loop feedback system. This rigorous evaluation approach has confirmed an 89% accuracy rate on domain-specific queries, representing a substantial improvement over baseline performance.\n",
    "\n",
    "Looking ahead, we're exploring several promising avenues for further enhancement. Our preliminary work on multi-vector indexing shows potential for handling complex, multi-hop queries more effectively. We're also investigating advanced context compression techniques that could enable more efficient processing of longer documents while maintaining retrieval accuracy. Initial results suggest these approaches could push our accuracy rates even higher.\n",
    "\n",
    "I look forward to presenting our detailed findings at next week's NLP conference, where we can discuss potential collaborations and share insights with the broader research community.\n",
    "\n",
    "Best regards,\n",
    "Pavan\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0664cba1-437d-4073-9877-b34ff96335f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:41:49.869260Z",
     "start_time": "2025-02-07T16:41:43.453046Z"
    },
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary:**\n",
      "\n",
      "The email is from Pavan, who shares their team's research findings on optimizing Retrieval Augmented Generation (RAG) pipelines. They report significant breakthroughs in improving accuracy and reducing hallucination rates through various enhancements to the RAG architecture, including chunking optimization, vector database layer improvements, query processing advancements, and preliminary work on multi-vector indexing and context compression.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* The team implemented semantic chunking that preserves contextual meaning, resulting in a 15% improvement in retrieval precision.\n",
      "* A hybrid search architecture combining dense and sparse vectors reduced hallucination rates by 27%.\n",
      "* An advanced query expansion system using a fine-tuned T5 model improved answer relevance by 32%.\n",
      "* A comprehensive evaluation framework with over 5,000 carefully curated question-answer pairs validated the improvements, achieving an 89% accuracy rate.\n",
      "* Preliminary work on multi-vector indexing and context compression shows potential for further enhancement.\n",
      "\n",
      "**What did the author say about RAG process?**\n",
      "\n",
      "The author mentions that their team has made significant breakthroughs in improving accuracy and reducing hallucination rates through systematic enhancements across multiple components of the RAG architecture. They highlight specific improvements, such as chunking optimization, vector database layer enhancements, query processing advancements, and preliminary work on multi-vector indexing and context compression.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points in bullets.\n",
    "What did the author say about RAG process?:\n",
    "\n",
    "email: {email}\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd077badf3d5da2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d32b6c3-c49d-4297-b58d-65bfceeda14e",
   "metadata": {},
   "source": [
    "### Providing New Information in the Prompt\n",
    "- A model's knowledge of the world ends at the moment of its training - so it won't know about more recent events.\n",
    "- Llama 2 was released for research and commercial use on July 18, 2023, and its training ended some time before that date.\n",
    "- Ask the model about an event, in this case, FIFA Women's World Cup 2023, which started on July 20, 2023, and see how the model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e29e6b-706e-438c-8d56-39bbf23cb94c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:45:20.763783Z",
     "start_time": "2025-02-07T16:45:19.310447Z"
    },
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have information on the winner of the 2023 Women's World Cup as my knowledge cutoff is December 2023, and I do not have real-time updates. However, I can suggest checking with a reliable news source or the official FIFA website for the latest information on the tournament.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Who won the 2023 Women's World Cup?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f959e08-e682-4ec0-8401-c4ed4d791485",
   "metadata": {},
   "source": [
    "- As you can see, the model still thinks that the tournament is yet to be played, even though you are now in 2024!\n",
    "- Another thing to **note** is, July 18, 2023 was the date the model was released to public, and it was trained even before that, so it only has information upto that point. The response says, \"the final match is scheduled to take place in July 2023\", but the final match was played on August 20, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30874cc3-73d7-412a-9dc5-1baa6aabf1b7",
   "metadata": {},
   "source": [
    "- You can provide the model with information about recent events, in this case text from Wikipedia about the 2023 Women's World Cup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cbe1b18-be2a-4f77-9911-c243e571226c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:45:41.326357Z",
     "start_time": "2025-02-07T16:45:41.321908Z"
    },
    "height": 149
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "The 2023 FIFA Women's World Cup (Māori: Ipu Wahine o te Ao FIFA i 2023)[1] was the ninth edition of the FIFA Women's World Cup, the quadrennial international women's football championship contested by women's national teams and organised by FIFA. The tournament, which took place from 20 July to 20 August 2023, was jointly hosted by Australia and New Zealand.[2][3][4] It was the first FIFA Women's World Cup with more than one host nation, as well as the first World Cup to be held across multiple confederations, as Australia is in the Asian confederation, while New Zealand is in the Oceanian confederation. It was also the first Women's World Cup to be held in the Southern Hemisphere.[5]\n",
    "This tournament was the first to feature an expanded format of 32 teams from the previous 24, replicating the format used for the men's World Cup from 1998 to 2022.[2] The opening match was won by co-host New Zealand, beating Norway at Eden Park in Auckland on 20 July 2023 and achieving their first Women's World Cup victory.[6]\n",
    "Spain were crowned champions after defeating reigning European champions England 1–0 in the final. It was the first time a European nation had won the Women's World Cup since 2007 and Spain's first title, although their victory was marred by the Rubiales affair.[7][8][9] Spain became the second nation to win both the women's and men's World Cup since Germany in the 2003 edition.[10] In addition, they became the first nation to concurrently hold the FIFA women's U-17, U-20, and senior World Cups.[11] Sweden would claim their fourth bronze medal at the Women's World Cup while co-host Australia achieved their best placing yet, finishing fourth.[12] Japanese player Hinata Miyazawa won the Golden Boot scoring five goals throughout the tournament. Spanish player Aitana Bonmatí was voted the tournament's best player, winning the Golden Ball, whilst Bonmatí's teammate Salma Paralluelo was awarded the Young Player Award. England goalkeeper Mary Earps won the Golden Glove, awarded to the best-performing goalkeeper of the tournament.\n",
    "Of the eight teams making their first appearance, Morocco were the only one to advance to the round of 16 (where they lost to France; coincidentally, the result of this fixture was similar to the men's World Cup in Qatar, where France defeated Morocco in the semi-final). The United States were the two-time defending champions,[13] but were eliminated in the round of 16 by Sweden, the first time the team had not made the semi-finals at the tournament, and the first time the defending champions failed to progress to the quarter-finals.[14]\n",
    "Australia's team, nicknamed the Matildas, performed better than expected, and the event saw many Australians unite to support them.[15][16][17] The Matildas, who beat France to make the semi-finals for the first time, saw record numbers of fans watching their games, their 3–1 loss to England becoming the most watched television broadcast in Australian history, with an average viewership of 7.13 million and a peak viewership of 11.15 million viewers.[18]\n",
    "It was the most attended edition of the competition ever held.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8b9d517-23f8-4468-9fc7-e47322d5b2fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:45:44.972518Z",
     "start_time": "2025-02-07T16:45:43.026852Z"
    },
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain won the 2023 Women's World Cup by defeating England 1-0 in the final.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the following context, who won the 2023 Women's World cup?\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdadbc9-e7ae-4281-8dc7-3f3feaa62def",
   "metadata": {},
   "source": [
    "### Try it Yourself!\n",
    "\n",
    "Try asking questions of your own! Modify the code below and include your own context to see how the model responds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71662250-4f89-4816-8752-96bb6749296a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:46:04.838696Z",
     "start_time": "2025-02-07T16:46:04.234096Z"
    },
    "height": 251
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "Given the following context,\n",
      "<your query here>\n",
      "\n",
      "context: \n",
      "<paste context in here>\n",
      "\n",
      "[/INST]\n",
      "\n",
      "model: llama3.2:latest\n",
      "Please go ahead and provide the context, and I'll do my best to assist you with your query.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "<paste context in here>\n",
    "\"\"\"\n",
    "query = \"<your query here>\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following context,\n",
    "{query}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt,verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51371f2d-9204-41b1-930c-2bd99888c5ab",
   "metadata": {},
   "source": [
    "### Chain-of-thought Prompting\n",
    "- LLMs can perform better at reasoning and logic problems if you ask them to break the problem down into smaller steps. This is known as **chain-of-thought** prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4883c94b-33ae-450c-a587-abb781b996ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:55:55.718723Z",
     "start_time": "2025-02-07T16:55:51.910510Z"
    },
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze the situation:\n",
      "\n",
      "* We have 15 people in total.\n",
      "* Two people have cars, and each car can seat 5 people. This means that the two cars can seat a maximum of 10 people (2 x 5 = 10).\n",
      "* Since we need to accommodate all 15 people, we'll use the motorcycles as well.\n",
      "\n",
      "We have 5 people left who don't fit in the cars. These 5 people will be transported by motorcycle.\n",
      "\n",
      "Since each motorcycle can fit 2 people, we'll need at least 3 motorcycles to transport these 5 people (5 / 2 = 2.5, so we round up to 3).\n",
      "\n",
      "So, to summarize:\n",
      "\n",
      "* The two cars will seat 10 people.\n",
      "* Three motorcycles will seat the remaining 5 people.\n",
      "\n",
      "Yes, all 15 people can get to the AI Conference by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a AI Conference.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the AI Conference by car or motorcycle?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3214335-2a49-4e52-b5c1-46b2674826ca",
   "metadata": {},
   "source": [
    "- Modify the prompt to ask the model to \"think step by step\" about the math problem you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d049009-4b88-4dd3-9fb6-f81870a86425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:58:31.042869Z",
     "start_time": "2025-02-07T16:58:26.550436Z"
    },
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down the problem step by step:\n",
      "\n",
      "1. We have 15 people in total, and two of them have cars that can seat 5 people each. This means we have a total capacity of 10 people for the cars (2 cars x 5 people each).\n",
      "2. Since there are only 15 people in total, and we already have a capacity of 10 people for the cars, it's not possible to fit all 15 people into the cars.\n",
      "3. Now, let's consider the motorcycles. We have two people who own motorcycles that can seat 2 people each. This means we have a total capacity of 4 people for the motorcycles (2 motorcycles x 2 people each).\n",
      "4. Since we still need to accommodate 5 more people after using the cars and motorcycles, it's not possible to fit all 15 people by car or motorcycle alone.\n",
      "\n",
      "Therefore, unfortunately, it is not possible to get all 15 people to the AI Conference by car or motorcycle alone.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a AI Conference.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the AI Conference by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761a4f0-1806-4c9d-b2db-36a213b588c1",
   "metadata": {},
   "source": [
    "- Provide the model with additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999a4468-5b1a-4a80-a3fb-b2add7912f99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:58:47.136750Z",
     "start_time": "2025-02-07T16:58:37.291381Z"
    },
    "height": 285
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if everyone can get to the AI Conference by car or motorcycle, we'll break down the problem into manageable steps.\n",
      "\n",
      "Step 1: Calculate the total number of people who need transportation\n",
      "There are 15 people in total. Since two have cars and two have motorcycles, that's a total of 4 vehicles. However, we don't know how many people will use each vehicle yet.\n",
      "\n",
      "Step 2: Determine the capacity of all vehicles combined\n",
      "Each car can seat 5 people, so 2 cars can seat 10 people (2 x 5). Each motorcycle can fit 2 people, so 2 motorcycles can fit 4 people (2 x 2).\n",
      "\n",
      "Total capacity = Car capacity + Motorcycle capacity\n",
      "= 10 + 4\n",
      "= 14\n",
      "\n",
      "Step 3: Compare the total capacity to the number of people who need transportation\n",
      "We have a total of 15 people who need transportation, but our vehicles can only seat 14 people. This means we're short one person.\n",
      "\n",
      "Since we can't fit everyone in the available vehicles, we'll analyze further:\n",
      "\n",
      "Step 4: Consider alternative arrangements for the remaining person\n",
      "The two cars can each take up to 5 people, leaving some capacity unused (10 - 2 = 8). The two motorcycles can also be used to transport more people. However, since each motorcycle can only fit 2 people, we'd need at least one car to accommodate any additional passengers.\n",
      "\n",
      "Step 5: Determine the feasibility of using cars and motorcycles together\n",
      "Given that we have a limited number of vehicles and seats available, it's unlikely that everyone can be transported by car or motorcycle alone. However, if we combine both modes of transportation, we might be able to accommodate all 15 people.\n",
      "\n",
      "One possible arrangement could be:\n",
      "\n",
      "- Two cars (seating 10 people) with the remaining 5 people taking motorcycles.\n",
      "- The two motorcycles would need to carry the last 5 people, which is not feasible since each motorcycle can only fit 2 people.\n",
      "\n",
      "However, we can adjust this by having one car seat fewer people and using both motorcycles to transport more passengers. For example:\n",
      "\n",
      "- One car (seating 4 people) with 3 people taking motorcycles.\n",
      "- The two motorcycles would then carry the remaining 5 people.\n",
      "\n",
      "This arrangement allows us to accommodate all 15 people.\n",
      "\n",
      "Answer: Yes, everyone can get to the AI Conference by car or motorcycle if we use a combination of both modes of transportation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a AI Conference.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the AI Conference by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "Explain each intermediate step.\n",
    "Only when you are done with all your steps,\n",
    "provide the answer based on your intermediate steps.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ebad47-3202-45a6-97f3-3f693e43afeb",
   "metadata": {},
   "source": [
    "- The order of instructions matters!\n",
    "- Ask the model to \"answer first\" and \"explain later\" to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5756ee56-eaa7-43ab-8531-4f575014b26a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T16:58:55.418785Z",
     "start_time": "2025-02-07T16:58:52.125839Z"
    },
    "height": 268
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes.\n",
      "\n",
      "Here's the step-by-step explanation:\n",
      "\n",
      "1. We have two cars, each seating 5 people, so we can seat up to 10 people in the cars (2 cars x 5 people/car).\n",
      "2. Since there are only 15 people in total and 10 seats available in the cars, we need to find a way to transport the remaining 5 people.\n",
      "3. We have two motorcycles, each seating 2 people, so we can seat up to 4 people on the motorcycles (2 motorcycles x 2 people/motorcycle).\n",
      "4. Since there are only 5 people left and 4 seats available on the motorcycles, we can fit all 5 people on the motorcycles.\n",
      "\n",
      "Therefore, it is possible for all 15 people to get to the AI Conference by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a AI Conference.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the AI Conference by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166a63a-ad56-4ee9-92b8-b198ed9b172b",
   "metadata": {},
   "source": [
    "- Since LLMs predict their answer one token at a time, the best practice is to ask them to think step by step, and then only provide the answer after they have explained their reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da590bee34e3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
